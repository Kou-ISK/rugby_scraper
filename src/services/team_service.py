"""
å…¨è©¦åˆãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ãƒãƒ¼ãƒ åã‚’æŠ½å‡ºã—ã¦teams.jsonãƒã‚¹ã‚¿ã‚’ç”Ÿæˆ

ã€é‡è¦ã€‘IDå®‰å®šæ€§ä¿è¨¼:
- æ—¢å­˜ãƒãƒ¼ãƒ ã®IDã¯çµ¶å¯¾ã«å¤‰æ›´ã—ãªã„
- æ–°ãƒãƒ¼ãƒ ã®ã¿æ–°IDã‚’æ¡ç•ªï¼ˆæœ€å¤§ID+1ã‹ã‚‰ï¼‰
- ãƒãƒ¼ãƒ åã§ã®ç…§åˆï¼ˆå¤§æ–‡å­—å°æ–‡å­—ç„¡è¦–ã€ç©ºç™½æ­£è¦åŒ–ï¼‰

ã€ãƒ­ã‚´å–å¾—ã€‘:
- TheSportsDB APIçµ±åˆ
- ãƒãƒ¼ãƒ åâ†’ãƒ­ã‚´URLè‡ªå‹•å–å¾—

ã€é‡è¤‡æ¤œå‡ºã€‘:
- ã‚¹ãƒãƒ³ã‚µãƒ¼åé•ã„ã®åŒä¸€ãƒãƒ¼ãƒ æ¤œå‡º
- ãƒãƒ¼ã‚¸æ¨å¥¨ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
"""

import json
import re
from pathlib import Path
from collections import defaultdict
import requests
import time

ROOT = Path(__file__).resolve().parents[2]
MATCHES_DIR = ROOT / "data" / "matches"
TEAMS_JSON = ROOT / "data" / "teams.json"
DUPLICATES_REPORT = ROOT / "data" / "team_duplicates_report.json"

# TheSportsDB APIè¨­å®š
THESPORTSDB_API_KEY = "3"  # ç„¡æ–™ãƒ—ãƒ©ãƒ³ã®ã‚­ãƒ¼
THESPORTSDB_BASE_URL = "https://www.thesportsdb.com/api/v1/json"

# å¤§ä¼šIDãƒ»ç•¥ç§°ãƒãƒƒãƒ”ãƒ³ã‚°ï¼ˆæ–°å½¢å¼å¯¾å¿œï¼‰
COMPETITION_IDS = {
    "six-nations": "m6n",
    "six-nations-women": "w6n",
    "six-nations-u20": "u6n",
    "league-one": "jrlo_div1",  # Division 1ãŒãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
    "top14": "t14",
    "gallagher-premiership": "gp",
    "urc": "urc",
    "epcr-champions": "ecc",
    "epcr-challenge": "ech",
    "super-rugby-pacific": "srp",
    "rugby-championship": "trc",
    "autumn-nations-series": "ans",
    "world-rugby-internationals": "wri",
}

# å›½éš›å¤§ä¼šã®å®šç¾©ï¼ˆBaseScraper.INTERNATIONAL_COMPETITIONSã¨åŒæœŸï¼‰
INTERNATIONAL_COMPETITIONS = {
    "m6n": "M",      # Six Nations (Men)
    "w6n": "W",      # Six Nations (Women)
    "u6n": "U20",    # Six Nations U20
    "trc": "M",      # The Rugby Championship
    "ans": "M",      # Autumn Nations Series
    "wri": "M",      # World Rugby Internationals
}

# å›½ã‚³ãƒ¼ãƒ‰ãƒãƒƒãƒ”ãƒ³ã‚°ï¼ˆBaseScraper.COUNTRY_CODESã¨åŒæœŸï¼‰
COUNTRY_CODES = {
    # Six Nations
    "ENGLAND": "ENG", "England": "ENG", "ENG": "ENG",
    "FRANCE": "FRA", "France": "FRA", "FRA": "FRA",
    "IRELAND": "IRE", "Ireland": "IRE", "IRE": "IRE",
    "ITALY": "ITA", "Italy": "ITA", "ITA": "ITA",
    "SCOTLAND": "SCO", "Scotland": "SCO", "SCO": "SCO",
    "WALES": "WAL", "Wales": "WAL", "WAL": "WAL",
    # The Rugby Championship
    "ARGENTINA": "ARG", "Argentina": "ARG", "ARG": "ARG",
    "AUSTRALIA": "AUS", "Australia": "AUS", "AUS": "AUS",
    "NEW ZEALAND": "NZL", "New Zealand": "NZL", "NZL": "NZL",
    "SOUTH AFRICA": "RSA", "South Africa": "RSA", "RSA": "RSA",
    # Others
    "JAPAN": "JPN", "Japan": "JPN", "JPN": "JPN",
    "FIJI": "FIJ", "Fiji": "FIJ", "FIJ": "FIJ",
    "CHILE": "CHI", "Chile": "CHI", "CHI": "CHI",
    "USA": "USA",
}


def normalize_team_name(name):
    """ãƒãƒ¼ãƒ åã‚’æ­£è¦åŒ–ï¼ˆæ¯”è¼ƒç”¨ï¼‰"""
    if not name:
        return ""
    # å¤§æ–‡å­—åŒ–ã€ç©ºç™½æ­£è¦åŒ–ã€è¨˜å·é™¤å»
    normalized = name.upper().strip()
    normalized = re.sub(r'\s+', ' ', normalized)
    normalized = re.sub(r'[^\w\s]', '', normalized)
    return normalized


def extract_national_team_variant_suffix(team_name):
    """ä»£è¡¨ãƒãƒ¼ãƒ ã®ãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³æ¥å°¾è¾ã‚’æŠ½å‡ºï¼ˆBaseScraperã¨åŒã˜ãƒ­ã‚¸ãƒƒã‚¯ï¼‰"""
    if not team_name:
        return ""
    
    # Aä»£è¡¨ã€XVä»£è¡¨ç­‰ã®ãƒ‘ã‚¿ãƒ¼ãƒ³
    variant_patterns = [
        (r'\s+A$', 'A'),
        (r'\s+XV$', 'XV'),
        (r'\s+Barbarians$', 'Barbarians'),
        (r'\s+Development$', 'Dev'),
    ]
    
    for pattern, suffix in variant_patterns:
        if re.search(pattern, team_name, re.IGNORECASE):
            return suffix
    
    return ""


def generate_national_team_id(team_name, comp_id):
    """å›½ä»£è¡¨ãƒãƒ¼ãƒ IDã‚’ç”Ÿæˆï¼ˆBaseScraper._generate_national_team_idã¨åŒã˜ãƒ­ã‚¸ãƒƒã‚¯ï¼‰
    
    Returns:
        NT-{M|W|U20}-{ENG|FRA|...}[-{A|XV|...}] å½¢å¼ã®IDã€ã¾ãŸã¯ç©ºæ–‡å­—åˆ—
    """
    if not team_name or comp_id not in INTERNATIONAL_COMPETITIONS:
        return ""
    
    # 1. ã‚«ãƒ†ã‚´ãƒªå–å¾—
    category = INTERNATIONAL_COMPETITIONS[comp_id]
    
    # 2. ãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³æŠ½å‡º
    variant = extract_national_team_variant_suffix(team_name)
    base_name = re.sub(r'\s+(A|XV|Barbarians|Development)$', '', team_name, flags=re.IGNORECASE).strip()
    
    # 3. å›½ã‚³ãƒ¼ãƒ‰å–å¾—
    country_code = COUNTRY_CODES.get(base_name, "")
    if not country_code:
        # å¤§æ–‡å­—åŒ–ã§å†è©¦è¡Œ
        country_code = COUNTRY_CODES.get(base_name.upper(), "")
    
    if not country_code:
        return ""
    
    # 4. IDç”Ÿæˆ
    if variant:
        return f"NT-{category}-{country_code}-{variant}"
    else:
        return f"NT-{category}-{country_code}"


def get_base_team_name(name):
    """ã‚¹ãƒãƒ³ã‚µãƒ¼åã‚’é™¤å»ã—ã¦ãƒ™ãƒ¼ã‚¹ãƒãƒ¼ãƒ åã‚’å–å¾—"""
    # ã‚¹ãƒãƒ³ã‚µãƒ¼åãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆBaseScraper.SPONSOR_PATTERNSã¨åŒæœŸï¼‰
    sponsor_patterns = [
        # å…ˆé ­ã®ã‚¹ãƒãƒ³ã‚µãƒ¼å
        r'^DHL\s+',
        r'^ISUZU\s+',
        r'^GALLAGHER\s+',
        # æœ«å°¾ã®ã‚¹ãƒãƒ³ã‚µãƒ¼å
        r'\s+GIO$',
        r'\s+HBF$',
        r'\s+FMG$',
        r'\s+SKY$',
        r'\s+DHL$',
        r'\s+ISUZU$',
        r'\s+GALLAGHER$',
        r'\s+4R$',
        r'\s+FOUR\s+R$',
        r'\s+CHURCHILL$',
        r'\s+MCLEAN$',
        r'\s+HIF$',
        r'\s+HFC\s+BANK$',
    ]
    
    base_name = name
    for pattern in sponsor_patterns:
        base_name = re.sub(pattern, '', base_name, flags=re.IGNORECASE)
    
    return base_name.strip()


def fetch_team_logo(team_name, comp_id):
    """TheSportsDB APIã‹ã‚‰ãƒãƒ¼ãƒ ãƒ­ã‚´ã‚’å–å¾—"""
    try:
        # ãƒ™ãƒ¼ã‚¹ãƒãƒ¼ãƒ åã‚’ä½¿ç”¨
        search_name = get_base_team_name(team_name)
        
        url = f"{THESPORTSDB_BASE_URL}/{THESPORTSDB_API_KEY}/searchteams.php"
        params = {"t": search_name}
        
        response = requests.get(url, params=params, timeout=10)
        response.raise_for_status()
        
        data = response.json()
        teams = data.get("teams")
        
        if teams and len(teams) > 0:
            team = teams[0]
            logo_url = team.get("strTeamBadge") or team.get("strTeamLogo") or ""
            badge_url = team.get("strTeamBanner") or ""
            
            # APIãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾ç­–
            time.sleep(0.5)
            
            return logo_url, badge_url
        
        return "", ""
    
    except Exception as e:
        print(f"    âš ï¸ ãƒ­ã‚´å–å¾—ã‚¨ãƒ©ãƒ¼ ({team_name}): {e}")
        return "", ""


def load_existing_teams():
    """æ—¢å­˜teams.jsonã‚’èª­ã¿è¾¼ã¿"""
    if not TEAMS_JSON.exists():
        return {}
    
    with open(TEAMS_JSON, 'r', encoding='utf-8') as f:
        return json.load(f)


def extract_teams_from_matches():
    """å…¨è©¦åˆãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ãƒãƒ¼ãƒ åã‚’æŠ½å‡º"""
    teams_by_comp = defaultdict(set)
    
    for comp_dir in sorted(MATCHES_DIR.iterdir()):
        if not comp_dir.is_dir():
            continue
        
        comp_id = comp_dir.name
        print(f"\nå¤§ä¼š: {comp_id}")
        
        for match_file in sorted(comp_dir.glob("*.json")):
            try:
                with open(match_file, 'r', encoding='utf-8') as f:
                    matches = json.load(f)
                
                for match in matches:
                    home_team = match.get("home_team", "").strip()
                    away_team = match.get("away_team", "").strip()
                    
                    if home_team:
                        teams_by_comp[comp_id].add(home_team)
                    if away_team:
                        teams_by_comp[comp_id].add(away_team)
            
            except Exception as e:
                print(f"  âš ï¸ ã‚¨ãƒ©ãƒ¼ ({match_file.name}): {e}")
    
    # setã‚’sortedãƒªã‚¹ãƒˆã«å¤‰æ›
    for comp_id in teams_by_comp:
        teams_by_comp[comp_id] = sorted(teams_by_comp[comp_id])
        print(f"  {comp_id}: {len(teams_by_comp[comp_id])}ãƒãƒ¼ãƒ ")
    
    return dict(teams_by_comp)


def detect_duplicates(teams_by_comp):
    """é‡è¤‡ãƒãƒ¼ãƒ ã‚’æ¤œå‡ºï¼ˆã‚¹ãƒãƒ³ã‚µãƒ¼åé•ã„ç­‰ï¼‰"""
    duplicates = []
    
    for comp_id, team_names in teams_by_comp.items():
        # ãƒ™ãƒ¼ã‚¹åã§ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
        base_to_teams = defaultdict(list)
        
        for name in team_names:
            base_name = normalize_team_name(get_base_team_name(name))
            if base_name:
                base_to_teams[base_name].append(name)
        
        # 2ã¤ä»¥ä¸Šã‚ã‚‹ã‚‚ã®ãŒé‡è¤‡å€™è£œ
        for base_name, names in base_to_teams.items():
            if len(names) > 1:
                duplicates.append({
                    "competition_id": comp_id,
                    "base_name": base_name,
                    "variations": names,
                    "suggestion": f"çµ±åˆæ¨å¥¨: {names[0]} ã‚’ä»£è¡¨ã¨ã™ã‚‹"
                })
    
    return duplicates


def generate_team_master(teams_by_comp, existing_teams, fetch_logos=True):
    """ãƒãƒ¼ãƒ ãƒã‚¹ã‚¿ã‚’ç”Ÿæˆï¼ˆIDå®‰å®šæ€§ä¿è¨¼ï¼‰"""
    
    # æ—¢å­˜ãƒãƒ¼ãƒ ã‚’ãƒãƒ¼ãƒ åã§ç´¢å¼•åŒ–ï¼ˆæ­£è¦åŒ–åã§ãƒãƒƒãƒ”ãƒ³ã‚°ï¼‰
    existing_by_name = {}
    existing_by_id = {}
    
    for team_id, team_data in existing_teams.items():
        name = team_data.get("name", "")
        comp_id = team_data.get("competition_id", "")
        
        if name and comp_id:
            key = (comp_id, normalize_team_name(name))
            existing_by_name[key] = team_id
            existing_by_id[team_id] = team_data
    
    # å¤§ä¼šåˆ¥ã®æœ€å¤§IDç•ªå·ã‚’å–å¾—
    max_id_by_comp = defaultdict(int)
    for team_id in existing_teams.keys():
        if '-' in team_id:
            comp_prefix, num_str = team_id.rsplit('-', 1)
            try:
                num = int(num_str)
                max_id_by_comp[comp_prefix] = max(max_id_by_comp[comp_prefix], num)
            except ValueError:
                pass
    
    # æ–°ãƒãƒ¼ãƒ ãƒã‚¹ã‚¿
    new_teams = {}
    added_count = 0
    preserved_count = 0
    
    for comp_id, team_names in sorted(teams_by_comp.items()):
        print(f"\nå¤§ä¼š: {comp_id}")
        
        for team_name in team_names:
            # å›½éš›å¤§ä¼šã®å ´åˆã€NT-*å½¢å¼ã®IDã‚’ç”Ÿæˆ
            if comp_id in INTERNATIONAL_COMPETITIONS:
                team_id = generate_national_team_id(team_name, comp_id)
                
                if not team_id:
                    # IDç”Ÿæˆå¤±æ•—ï¼ˆå›½ã‚³ãƒ¼ãƒ‰ãŒè¦‹ã¤ã‹ã‚‰ãªã„ç­‰ï¼‰
                    print(f"  âš ï¸ {team_name}: å›½ä»£è¡¨IDç”Ÿæˆå¤±æ•—ï¼ˆã‚¹ã‚­ãƒƒãƒ—ï¼‰")
                    continue
                
                # æ—¢å­˜ãƒãƒ¼ãƒ ç¢ºèª
                if team_id in existing_by_id:
                    # æ—¢å­˜ãƒãƒ¼ãƒ : IDã‚’ä¿æŒ
                    new_teams[team_id] = existing_by_id[team_id]
                    preserved_count += 1
                    print(f"  âœ“ {team_id}: {team_name} (æ—¢å­˜IDä¿æŒ)")
                else:
                    # æ–°è¦å›½ä»£è¡¨ãƒãƒ¼ãƒ 
                    logo_url = ""
                    badge_url = ""
                    if fetch_logos:
                        print(f"  ğŸ” {team_id}: {team_name} (ãƒ­ã‚´å–å¾—ä¸­...)")
                        logo_url, badge_url = fetch_team_logo(team_name, comp_id)
                        if logo_url:
                            print(f"    âœ… ãƒ­ã‚´å–å¾—æˆåŠŸ")
                        else:
                            print(f"    âš ï¸ ãƒ­ã‚´æœªå–å¾—")
                    else:
                        print(f"  â• {team_id}: {team_name} (æ–°è¦è¿½åŠ )")
                    
                    new_teams[team_id] = {
                        "id": team_id,
                        "competition_id": comp_id,
                        "name": team_name,
                        "name_ja": "",
                        "short_name": team_name,
                        "country": "",
                        "division": "",
                        "logo_url": logo_url,
                        "badge_url": badge_url,
                    }
                    added_count += 1
            
            else:
                # ã‚¯ãƒ©ãƒ–ãƒãƒ¼ãƒ : æ—¢å­˜ãƒ­ã‚¸ãƒƒã‚¯ï¼ˆ{comp_id}-{num}å½¢å¼ï¼‰
                # æ—¢å­˜ãƒãƒ¼ãƒ ç¢ºèª
                key = (comp_id, normalize_team_name(team_name))
                
                if key in existing_by_name:
                    # æ—¢å­˜ãƒãƒ¼ãƒ : IDã‚’ä¿æŒ
                    team_id = existing_by_name[key]
                    new_teams[team_id] = existing_by_id[team_id]
                    preserved_count += 1
                    print(f"  âœ“ {team_id}: {team_name} (æ—¢å­˜IDä¿æŒ)")
                
                else:
                    # æ–°ãƒãƒ¼ãƒ : æ–°IDæ¡ç•ª
                    max_id_by_comp[comp_id] += 1
                    team_id = f"{comp_id}-{max_id_by_comp[comp_id]}"
                    
                    # ãƒ­ã‚´å–å¾—
                    logo_url = ""
                    badge_url = ""
                    if fetch_logos:
                        print(f"  ğŸ” {team_id}: {team_name} (ãƒ­ã‚´å–å¾—ä¸­...)")
                        logo_url, badge_url = fetch_team_logo(team_name, comp_id)
                        if logo_url:
                            print(f"    âœ… ãƒ­ã‚´å–å¾—æˆåŠŸ")
                        else:
                            print(f"    âš ï¸ ãƒ­ã‚´æœªå–å¾—")
                    else:
                        print(f"  â• {team_id}: {team_name} (æ–°è¦è¿½åŠ )")
                    
                    new_teams[team_id] = {
                        "id": team_id,
                        "competition_id": comp_id,
                        "name": team_name,
                        "name_ja": "",
                        "short_name": team_name,
                        "country": "",
                        "division": "",
                        "logo_url": logo_url,
                        "badge_url": badge_url,
                    }
                    
                    added_count += 1
    
    return new_teams, added_count, preserved_count


def update_team_logos():
    """æ—¢å­˜ãƒãƒ¼ãƒ ã®ãƒ­ã‚´URLã‚’TheSportsDB APIã‹ã‚‰æ›´æ–°"""
    print("=" * 60)
    print("æ—¢å­˜ãƒãƒ¼ãƒ ã®ãƒ­ã‚´URLæ›´æ–°")
    print("=" * 60)
    
    # æ—¢å­˜teams.jsonèª­ã¿è¾¼ã¿
    existing_teams = load_existing_teams()
    print(f"\næ—¢å­˜teams.json: {len(existing_teams)}ãƒãƒ¼ãƒ ")
    
    updated_count = 0
    skipped_count = 0
    failed_count = 0
    
    for team_id, team_data in existing_teams.items():
        team_name = team_data.get("name", "")
        existing_logo = team_data.get("logo_url", "")
        
        # æ—¢ã«ãƒ­ã‚´URLãŒã‚ã‚‹å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
        if existing_logo:
            skipped_count += 1
            continue
        
        print(f"â³ {team_name}ã®ãƒ­ã‚´ã‚’å–å¾—ä¸­...")
        
        # TheSportsDB APIã‹ã‚‰å–å¾—
        logo_info = fetch_logo_from_thesportsdb(team_name)
        
        if logo_info.get("logo_url"):
            team_data["logo_url"] = logo_info["logo_url"]
            team_data["badge_url"] = logo_info.get("badge_url", "")
            updated_count += 1
            print(f"  âœ“ ãƒ­ã‚´URLå–å¾—æˆåŠŸ")
        else:
            failed_count += 1
            print(f"  âœ— ãƒ­ã‚´URLå–å¾—å¤±æ•—")
        
        # APIåˆ¶é™å¯¾ç­–ï¼ˆ1ç§’å¾…æ©Ÿï¼‰
        time.sleep(1)
    
    # ä¿å­˜
    with open(TEAMS_JSON, 'w', encoding='utf-8') as f:
        json.dump(existing_teams, f, ensure_ascii=False, indent=2)
        f.write("\n")
    
    print("\n" + "=" * 60)
    print("âœ… ãƒ­ã‚´URLæ›´æ–°å®Œäº†")
    print("=" * 60)
    print(f"æ›´æ–°: {updated_count}ãƒãƒ¼ãƒ ")
    print(f"ã‚¹ã‚­ãƒƒãƒ—ï¼ˆæ—¢å­˜ã‚ã‚Šï¼‰: {skipped_count}ãƒãƒ¼ãƒ ")
    print(f"å¤±æ•—: {failed_count}ãƒãƒ¼ãƒ ")


def main():
    print("=" * 60)
    print("ãƒãƒ¼ãƒ ãƒã‚¹ã‚¿æ›´æ–°ï¼ˆIDå®‰å®šæ€§ä¿è¨¼ + ãƒ­ã‚´å–å¾—ï¼‰")
    print("=" * 60)
    
    # æ—¢å­˜teams.jsonèª­ã¿è¾¼ã¿
    existing_teams = load_existing_teams()
    print(f"\næ—¢å­˜teams.json: {len(existing_teams)}ãƒãƒ¼ãƒ ")
    
    # å…¨è©¦åˆãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ãƒãƒ¼ãƒ æŠ½å‡º
    print("\nå…¨è©¦åˆãƒ‡ãƒ¼ã‚¿ã‚’ã‚¹ã‚­ãƒ£ãƒ³ä¸­...")
    teams_by_comp = extract_teams_from_matches()
    
    total_unique = sum(len(names) for names in teams_by_comp.values())
    print(f"\nâœ… æŠ½å‡ºå®Œäº†: {total_unique}ãƒ¦ãƒ‹ãƒ¼ã‚¯ãƒãƒ¼ãƒ ")
    
    # é‡è¤‡æ¤œå‡º
    print("\né‡è¤‡ãƒãƒ¼ãƒ æ¤œå‡ºä¸­...")
    duplicates = detect_duplicates(teams_by_comp)
    
    if duplicates:
        print(f"\nâš ï¸ {len(duplicates)}ä»¶ã®é‡è¤‡å€™è£œã‚’æ¤œå‡º")
        for dup in duplicates[:5]:  # æœ€åˆã®5ä»¶è¡¨ç¤º
            print(f"  {dup['competition_id']}: {', '.join(dup['variations'])}")
        
        # é‡è¤‡ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜
        with open(DUPLICATES_REPORT, 'w', encoding='utf-8') as f:
            json.dump(duplicates, f, ensure_ascii=False, indent=2)
            f.write("\n")
        print(f"\nè©³ç´°ãƒ¬ãƒãƒ¼ãƒˆ: {DUPLICATES_REPORT}")
    
    # ãƒãƒ¼ãƒ ãƒã‚¹ã‚¿ç”Ÿæˆï¼ˆãƒ­ã‚´å–å¾—æœ‰åŠ¹ï¼‰
    print("\nãƒãƒ¼ãƒ ãƒã‚¹ã‚¿ç”Ÿæˆä¸­ï¼ˆãƒ­ã‚´å–å¾—æœ‰åŠ¹ï¼‰...")
    print("â³ TheSportsDB APIå•ã„åˆã‚ã›ä¸­... ï¼ˆæ™‚é–“ãŒã‹ã‹ã‚Šã¾ã™ï¼‰")
    
    new_teams, added_count, preserved_count = generate_team_master(
        teams_by_comp, 
        existing_teams,
        fetch_logos=True  # ãƒ­ã‚´å–å¾—ON
    )
    
    # ä¿å­˜
    with open(TEAMS_JSON, 'w', encoding='utf-8') as f:
        json.dump(new_teams, f, ensure_ascii=False, indent=2)
        f.write("\n")
    
    print("\n" + "=" * 60)
    print("âœ… teams.json æ›´æ–°å®Œäº†")
    print("=" * 60)
    print(f"ç·ãƒãƒ¼ãƒ æ•°: {len(new_teams)}")
    print(f"  - æ—¢å­˜IDä¿æŒ: {preserved_count}")
    print(f"  - æ–°è¦è¿½åŠ : {added_count}")
    
    if duplicates:
        print(f"\nâš ï¸ é‡è¤‡å€™è£œ: {len(duplicates)}ä»¶")
        print(f"   ãƒ¬ãƒãƒ¼ãƒˆ: {DUPLICATES_REPORT}")


if __name__ == "__main__":
    main()
